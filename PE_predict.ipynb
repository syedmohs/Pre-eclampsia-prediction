{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4746516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "import numpy as np\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from statistics import mean\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler,label_binarize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, make_scorer,roc_curve, precision_recall_curve, auc\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import Lasso,LinearRegression\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.linear_model import Ridge\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import shap\n",
    "from sklearn.feature_selection import SelectKBest, RFE\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354ccf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the annotation file\n",
    "annotation_file = 'Human.GRCh38.p13.annot.tsv.gz'\n",
    "\n",
    "with gzip.open(annotation_file, 'rt') as f:\n",
    "    annotations = pd.read_csv(f, sep='\\t')\n",
    "\n",
    "# Ensure gene IDs are strings and strip whitespace\n",
    "annotations['GeneID'] = annotations['GeneID'].astype(str).str.strip()\n",
    "annotations['Symbol'] = annotations['Symbol'].str.strip()\n",
    "\n",
    "# Create a dictionary mapping from gene IDs to gene symbols\n",
    "id_to_symbol = dict(zip(annotations['GeneID'], annotations['Symbol']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330241be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ids = pd.read_csv(\"GSE234729.csv\")\n",
    "# Set the first column as the index and remove its name\n",
    "df_ids.set_index(df_ids.columns[0], inplace=True, drop=True)\n",
    "# Remove the name of the index\n",
    "df_ids.index.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d99751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace gene IDs with gene symbols in the columns, keeping original IDs if no match is found\n",
    "df_ids.columns = [id_to_symbol.get(gene, gene) for gene in df_ids.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06618b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the common genes file\n",
    "common_genes_df = pd.read_csv(\"common_genes.csv\")\n",
    "\n",
    "# Convert the common genes to a set for easy filtering\n",
    "common_genes = set(common_genes_df['CommonGenes'])\n",
    "# Apply the common genes mask to keep only the columns with common gene symbols\n",
    "df = df_ids.copy()[df_ids.columns.intersection(common_genes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bdf49c",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b614210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values: \",df.isna().all().sum())\n",
    "print(df['Pre-eclampsia'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531574e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1]\n",
    "\n",
    "# Extract the label column (the last column)\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode the labels into numerical values\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Replace the original label column with the encoded labels\n",
    "df[df.columns[-1]] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb70471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dfc4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = X_train.corr()\n",
    "# get upper triangle of correlation matrix\n",
    "\n",
    "upper = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# find features with correlation greater than 0.80\n",
    "\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.8)]\n",
    "\n",
    "# drop highly correlated features\n",
    "X_train.drop(to_drop, axis=1, inplace=True)\n",
    "X_test.drop(to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67c7636",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fed109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the base estimator\n",
    "base_estimator = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define the feature subset sizes to use\n",
    "n_features_list = [10, 20, 30, 40, 50]\n",
    "\n",
    "# Create dictionaries to store the resulting dataframes\n",
    "X_train_subsets = {}\n",
    "X_test_subsets = {}\n",
    "\n",
    "# Initialize progress bar using tqdm\n",
    "progress_bar = tqdm(n_features_list, desc=\"Performing RFE\", unit=\"subset\")\n",
    "\n",
    "# Perform RFE for each feature subset size\n",
    "for n_features in progress_bar:\n",
    "    # Record start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize RFE with the base estimator and number of features to select\n",
    "    rfe = RFE(estimator=base_estimator, n_features_to_select=n_features)\n",
    "    \n",
    "    # Fit the RFE model on the training data\n",
    "    rfe.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the selected features' mask (True if the feature is selected, False otherwise)\n",
    "    selected_features = rfe.support_\n",
    "    \n",
    "    # Create new X_train and X_test with the selected features\n",
    "    X_train_subset = X_train.loc[:, selected_features]\n",
    "    X_test_subset = X_test.loc[:, selected_features]\n",
    "    \n",
    "    # Store the new subsets in the dictionary\n",
    "    X_train_subsets[f'X_train_{n_features}_features'] = X_train_subset\n",
    "    X_test_subsets[f'X_test_{n_features}_features'] = X_test_subset\n",
    "    \n",
    "    # Calculate time taken\n",
    "    time_taken = time.time() - start_time\n",
    "    print(time_taken)\n",
    "    progress_bar.set_postfix({\"Time (s)\": round(time_taken, 2)})\n",
    "\n",
    "# Close progress bar\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dae3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Define directories to store the resulting CSVs\n",
    "train_save_path = \"train_feature_subsets/\"\n",
    "test_save_path = \"test_feature_subsets/\"\n",
    "os.makedirs(train_save_path, exist_ok=True)\n",
    "os.makedirs(test_save_path, exist_ok=True)\n",
    "\n",
    "# Save all the feature subsets after RFE is done\n",
    "for n_features in n_features_list:\n",
    "    # Get the subsets from the dictionaries\n",
    "    X_train_subset = X_train_subsets[f'X_train_{n_features}_features']\n",
    "    X_test_subset = X_test_subsets[f'X_test_{n_features}_features']\n",
    "    \n",
    "    # Define file names\n",
    "    train_csv_filename = f\"{train_save_path}X_train_{n_features}_features.csv\"\n",
    "    test_csv_filename = f\"{test_save_path}X_test_{n_features}_features.csv\"\n",
    "    \n",
    "    # Save to CSV\n",
    "    X_train_subset.to_csv(train_csv_filename, index=False)\n",
    "    X_test_subset.to_csv(test_csv_filename, index=False)\n",
    "\n",
    "print(\"All feature subsets have been saved to CSV files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9a2751",
   "metadata": {},
   "source": [
    "### Model development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d729df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the feature subset sizes to use\n",
    "n_features_list = [10, 20, 30, 40, 50]\n",
    "\n",
    "# Set directories where train and test CSV files are saved\n",
    "train_path = \"GSE234729_subsets/train_feature_subsets/\"\n",
    "test_path = \"GSE234729_subsets/test_feature_subsets/\"\n",
    "\n",
    "# Define parameter grids for each model\n",
    "param_grids = {\n",
    "    \"RandomForest\": {\n",
    "    \"n_estimators\": [50, 100, 150, 200],\n",
    "    \"max_depth\": [5, 10, 15, 20],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"max_features\": ['sqrt', 'log2', None]\n",
    "    },\n",
    "    \"DecisionTree\": {\n",
    "    \"max_depth\": [5, 10, 15, 20, None],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"max_features\": ['sqrt', 'log2', None]\n",
    "    },\n",
    "    \"SVM\": {\n",
    "    \"C\": [0.1, 1, 10, 100],\n",
    "    \"kernel\": [\"linear\", \"rbf\", \"poly\", \"sigmoid\"],\n",
    "    \"gamma\": [\"scale\", \"auto\"]\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "    \"n_estimators\": [50, 100, 150],\n",
    "    \"max_depth\": [3, 5, 7],\n",
    "    \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "    \"subsample\": [0.6, 0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.6, 0.8, 1.0]\n",
    "    },\n",
    "    \"KNN\": {\n",
    "    \"n_neighbors\": [1],\n",
    "    \"weights\": ['uniform', 'distance'],\n",
    "    \"metric\": ['euclidean', 'manhattan', 'chebyshev']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define models for each algorithm\n",
    "models = {\n",
    "    \"RandomForest\": RandomForestClassifier(random_state=42),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"SVM\": SVC(probability=True, random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(eval_metric='logloss', random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Dictionary to store trained models and results DataFrame\n",
    "trained_models = {}\n",
    "results = []\n",
    "\n",
    "# Loop over each feature subset and each model\n",
    "for n_features in n_features_list:\n",
    "    # Load training and test datasets\n",
    "    X_train = pd.read_csv(f\"{train_path}X_train_{n_features}_features.csv\")\n",
    "    X_test = pd.read_csv(f\"{test_path}X_test_{n_features}_features.csv\")\n",
    "    \n",
    "    \n",
    "    print(\"Max value across all features:\", X_train.max().max())\n",
    "    print(\"Min value across all features:\", X_train.min().min())\n",
    "\n",
    "    print(\"NaNs:\", np.isnan(X_train.values).sum())\n",
    "    print(\"Infs:\", np.isinf(X_train.values).sum())\n",
    "    print(\"Very large (>1e6):\", (np.abs(X_train.values) > 1e6).sum())\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nTraining {model_name} model with {n_features} features...\")\n",
    "\n",
    "        # Set up GridSearchCV for the current model\n",
    "        grid_search = GridSearchCV(\n",
    "            model, param_grid=param_grids[model_name], cv=5, scoring='accuracy', n_jobs=-1, verbose=1\n",
    "        )\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Get the best model and best parameters from GridSearchCV\n",
    "        best_model = grid_search.best_estimator_\n",
    "        best_params = grid_search.best_params_\n",
    "\n",
    "        # Store the model in the dictionary\n",
    "        trained_models[f\"{model_name}_{n_features}_features\"] = best_model\n",
    "\n",
    "        # Evaluate the model on the test set\n",
    "        y_pred = best_model.predict(X_test)\n",
    "\n",
    "        # Calculate confusion matrix and metrics\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "        precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "        sensitivity = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "        specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "\n",
    "        # Append results to the list\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Feature Count\": n_features,\n",
    "            \"Best Parameters\": best_params,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Sensitivity (Recall)\": sensitivity,\n",
    "            \"Specificity\": specificity,\n",
    "            \"Precision\": precision,\n",
    "            \"Confusion Matrix\": f\"TP={tp}, TN={tn}, FP={fp}, FN={fn}\"\n",
    "        })\n",
    "\n",
    "        # Print results for each feature subset and model\n",
    "        print(f\"Results for {model_name} with {n_features} features:\")\n",
    "        print(f\"Best Hyperparameters: {best_params}\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "        print(f\"Specificity: {specificity:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Confusion Matrix: TP={tp}, TN={tn}, FP={fp}, FN={fn}\")\n",
    "        print(\"-\" * 40)\n",
    "        # Plot the confusion matrix using Seaborn's heatmap\n",
    "        plt.figure(figsize=(3, 3))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "                    xticklabels=[f'Class {i}' for i in range(2)],\n",
    "                    yticklabels=[f'Class {i}' for i in range(2)],annot_kws={\"size\": 8, \"weight\": \"bold\"})\n",
    "        plt.title(f'{model_name}',fontweight='bold', fontsize=12)\n",
    "        plt.xlabel('Predicted', fontweight='bold', fontsize=10)  # Increase font size for x-label\n",
    "        plt.ylabel('True', fontweight='bold', fontsize=10)  # Increase font size for y-label\n",
    "        # Set the x and y tick labels to bold and increase their size\n",
    "        plt.xticks(fontweight='bold', fontsize=10)\n",
    "        plt.yticks(fontweight='bold', fontsize=10)\n",
    "        plt.show()\n",
    "\n",
    "# Convert results to a DataFrame and save to Excel\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_excel(\"model_evaluation_results.xlsx\", index=False)\n",
    "print(\"\\nAll results saved to 'model_evaluation_results.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888d5fde",
   "metadata": {},
   "source": [
    "### Feature ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c14da06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the common genes from the intersection CSV file\n",
    "common_genes_df = pd.read_csv(\"intersection_features.csv\")\n",
    "common_genes = common_genes_df[\"Common Genes\"].tolist()\n",
    "\n",
    "# Filter the GSE114691 dataset to only include columns from the common genes\n",
    "gse234729_features = df[common_genes]\n",
    "\n",
    "# Assume there's a target column named 'target' in GSE114691 dataset\n",
    "X = gse234729_features\n",
    "y = df[\"Pre-eclampsia\"]  # Replace 'target' with the actual target column name\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf_model.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# Create a DataFrame with features and their importance scores\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"importance\": importances\n",
    "})\n",
    "\n",
    "# Sort the features by importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "# Plot the top 10 features\n",
    "top_10_features = feature_importance_df.head(5)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top_10_features[\"feature\"], top_10_features[\"importance\"], color=\"skyblue\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Top 10 Important Features in Random Forest\")\n",
    "plt.show()\n",
    "\n",
    "# Save the ranked features to a CSV\n",
    "top_10_features.to_csv(\"GSE234729_feature_importance_ranking.csv\", index=False)\n",
    "print(\"Feature importance ranking saved to GSE234729_feature_importance_ranking.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92467f88",
   "metadata": {},
   "source": [
    "### SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8acbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select two models from the dictionary\n",
    "model_1_name = \"RandomForest_30_features\"  # Replace with your first model name\n",
    "model_2_name = \"XGBoost_50_features\"  # Replace with your second model name\n",
    "\n",
    "# Load the two selected models\n",
    "model_1 = trained_models[model_1_name]\n",
    "model_2 = trained_models[model_2_name]\n",
    "\n",
    "explainer = shap.TreeExplainer(model_2)\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "\n",
    "# Define the font properties\n",
    "title_font_properties = {'fontsize': 20, 'fontweight': 'bold'}\n",
    "label_font_properties = {'fontsize': 16, 'fontweight': 'bold'}\n",
    "\n",
    "# Function to format tick labels and adjust spacing\n",
    "def format_func(value, tick_number):\n",
    "    return f\"{value:.2f}\"  # Format to 2 decimal places\n",
    "\n",
    "plt.title(f\"RF SHAP\", **title_font_properties)\n",
    "\n",
    "shap.summary_plot(shap_values[1], X_train, show=False)\n",
    "\n",
    "plt.xlabel(plt.gca().get_xlabel(), **label_font_properties)\n",
    "plt.ylabel(plt.gca().get_ylabel(), **label_font_properties)\n",
    "\n",
    "plt.gca().xaxis.set_major_formatter(ticker.FuncFormatter(format_func))\n",
    "# Customize x and y tick labels\n",
    "plt.xticks(fontsize=12, fontweight='bold')\n",
    "plt.yticks(fontsize=12, fontweight='bold')\n",
    "\n",
    "# Customize color bar label\n",
    "cbar = plt.gcf().axes[-1]  # Get the color bar\n",
    "cbar.set_ylabel('Feature value', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Customize color bar tick labels\n",
    "cbar.set_yticklabels(['Low', 'High'], fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (new_env)",
   "language": "python",
   "name": "new_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
